{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahar\\anaconda3\\envs\\neuro\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.io\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import pywt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from lightning import Fabric\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "import lightning as L\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../../motor-imagery-classification-2024/\")\n",
    "\n",
    "from classification.loaders import EEGDataset,load_data\n",
    "from models.unet.eeg_unets import Unet,UnetConfig, BottleNeckClassifier, Unet1D\n",
    "from classification.classifiers import DeepClassifier , SimpleCSP\n",
    "from classification.loaders import subject_dataset\n",
    "from classification.open_bci_loaders import OpenBCIDataset,OpenBCISubject, load_files\n",
    "from ntd.networks import SinusoidalPosEmb\n",
    "from ntd.diffusion_model import Diffusion\n",
    "from ntd.utils.kernels_and_diffusion_utils import WhiteNoiseProcess\n",
    "\n",
    "from u_net_diffusion import DiffusionUnet, DiffusionUnetConfig\n",
    "from models.unet import base_eegnet\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FS = 256\n",
    "DEVICE = \"cuda\"\n",
    "sns.set_style(\"darkgrid\")\n",
    "SAVE_PATH = \"../../saved_models/EEGNet\"\n",
    "if not os.path.isdir(SAVE_PATH):\n",
    "\tos.makedirs(SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path d:\\Machine learning\\MI SSL\\motor-imagery-classification-2024\\models\\diffusion\n",
      "Saving new data\n",
      "(1984, 2, 512)\n",
      "(1984,)\n",
      "final data shape: (1984, 2, 512)\n",
      "Saving new data\n",
      "(992, 2, 512)\n",
      "(992,)\n",
      "final data shape: (992, 2, 512)\n"
     ]
    }
   ],
   "source": [
    "print(f\"path {os.getcwd()}\")\n",
    "files = load_files(\"../../data/collected_data/\")\n",
    "train_split = 2*[[\"train\"]]\n",
    "test_split = 2*[[\"test\"]]\n",
    "save_path = os.path.join(\"processed\",\"raw\")\n",
    "csp_save_path = os.path.join(\"processed\",\"data/collected_data/csp\")\n",
    "\n",
    "train_csp_dataset = OpenBCIDataset(\n",
    "\tsubject_splits=train_split,\n",
    "\tdataset=files,\n",
    "\tsave_paths=[csp_save_path],\n",
    "\tfake_data=None,\n",
    "\tdataset_type=OpenBCISubject,\n",
    "\tchannels=np.arange(0,2),\n",
    "\tsubject_channels=[\"ch2\",\"ch5\"],\n",
    "\tstride=25,\n",
    "\tepoch_length=512\n",
    ")\n",
    "test_csp_dataset = OpenBCIDataset(\n",
    "\tsubject_splits=test_split,\n",
    "\tdataset=files,\n",
    "\tsave_paths=[csp_save_path],\n",
    "\tfake_data=None,\n",
    "\tdataset_type=OpenBCISubject,\n",
    "\tchannels=np.arange(0,2),\n",
    "\tsubject_channels=[\"ch2\",\"ch5\"],\n",
    "\tstride=25,\n",
    "\tepoch_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | time_embed  | SinusoidalPosEmb | 0     \n",
      "1  | class_embed | Embedding        | 24    \n",
      "2  | conv1       | Conv2d           | 2.1 K \n",
      "3  | conv2       | Conv2d           | 400   \n",
      "4  | pooling2    | MaxPool2d        | 0     \n",
      "5  | embed2      | Embed            | 288   \n",
      "6  | embed3      | Embed            | 576   \n",
      "7  | conv3       | Conv2d           | 724   \n",
      "8  | bottle_conv | Conv1d           | 1.3 K \n",
      "9  | pooling3    | MaxPool2d        | 0     \n",
      "10 | out_proj    | Linear           | 1.0 K \n",
      "11 | decode1     | DecoderBlock     | 13.4 K\n",
      "12 | decode2     | DecoderBlock     | 52.5 K\n",
      "13 | out_conv    | Conv1d           | 65    \n",
      "--------------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "model = base_eegnet.EEGUNet(8,8,2,512)\n",
    "print(ModelSummary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 6E-4\n",
    "num_epochs = 180\n",
    "time_dim = 12\n",
    "decay_min = 2\n",
    "decay_max = 2\n",
    "activation_type = \"leaky_relu\"\n",
    "num_timesteps = 1000\n",
    "schedule = \"linear\"\n",
    "batch_size = 64\n",
    "# If the schedule is not cosine, we need to test the end_beta\n",
    "start_beta = 0.0001\n",
    "end_beta = 0.08\n",
    "\t\t\n",
    "train_loader = DataLoader(\n",
    "\ttrain_csp_dataset,\n",
    "\tbatch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fabric,\n",
    "\t\t  unet,\n",
    "\t\t  train_dataset,\n",
    "\t\t  num_epochs):\n",
    "\t\t\n",
    "\tnoise_sampler = WhiteNoiseProcess(1.0, 512)\n",
    "\n",
    "\tdiffusion_model = Diffusion(\n",
    "\t\tnetwork=unet,\n",
    "\t\tdiffusion_time_steps=num_timesteps,\n",
    "\t\tnoise_sampler=noise_sampler,\n",
    "\t\tmal_dist_computer=noise_sampler,\n",
    "\t\tschedule=schedule,\n",
    "\t\tstart_beta=start_beta,\n",
    "\t\tend_beta=end_beta,\n",
    "\t)\n",
    "\toptimizer = optim.AdamW(\n",
    "\t\tunet.parameters(),\n",
    "\t\tlr=lr,\n",
    "\t)\n",
    "\n",
    "\ttrain_loader = DataLoader(\n",
    "\t\ttrain_dataset,\n",
    "\t\tbatch_size,\n",
    "\t)\n",
    "\t\t\n",
    "\tdiffusion_model,optimizer = fabric.setup(diffusion_model,optimizer)\n",
    "\ttrain_loader = fabric.setup_dataloaders(train_loader)\n",
    "\n",
    "\tloss_per_epoch = []\n",
    "\n",
    "\tstop_counter = 0\n",
    "\tmin_delta = 0.05\n",
    "\ttolerance = 30\n",
    "\t\t\t\n",
    "\t\t# Train model\n",
    "\tfor i in range(num_epochs):\n",
    "\t\t\n",
    "\t\tepoch_loss = []\n",
    "\t\tfor batch in train_loader:\n",
    "\t\t\t\n",
    "\t\t\twith fabric.autocast():\n",
    "\t\t\t# Repeat the cue signal to match the signal length\n",
    "\t\t\t\t# print(batch[\"signal\"].shape)\n",
    "\t\t\t\tsignal,cue = batch\n",
    "\t\t\t\tcue = (cue + 1).to(signal.dtype)\n",
    "\t\t\t\tcond = cue.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 512).to(DEVICE)\n",
    "\t\t\t\t\n",
    "\t\t\t\tloss = diffusion_model.train_batch(signal.to(DEVICE), cond=cond,\n",
    "\t\t\t\t\t\t\t\t\t   p_uncond=0.15)\n",
    "\t\t\tloss = torch.mean(loss)\n",
    "\t\t\t\n",
    "\t\t\tepoch_loss.append(loss.item())\n",
    "\t\t\t\n",
    "\t\t\tfabric.backward(loss)\n",
    "\t\t\t# loss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\n",
    "\t\tepoch_loss = np.mean(epoch_loss)\n",
    "\t\tloss_per_epoch.append(epoch_loss)\n",
    "\t\t\n",
    "\t\tprint(f\"Epoch {i} loss: {epoch_loss}\")\n",
    "\n",
    "\t\tprint(f\"diff: {epoch_loss - min(loss_per_epoch)}\")\n",
    "\n",
    "\t\tif epoch_loss - min(loss_per_epoch) >= min_delta*min(loss_per_epoch):\n",
    "\t\t\tstop_counter += 1\n",
    "\t\tif stop_counter > tolerance:\n",
    "\t\t\tbreak\n",
    "\ttorch.save(diffusion_model.state_dict(),os.path.join(SAVE_PATH,\"unet_diff.pt\"))\n",
    "\ttorch.save(unet.state_dict(),os.path.join(SAVE_PATH,\"unet_state_dict.pt\"))\n",
    "\treturn diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    }
   ],
   "source": [
    "FABRIC = Fabric(accelerator=\"cuda\",precision=\"bf16-mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahar\\anaconda3\\envs\\neuro\\lib\\site-packages\\torch\\nn\\functional.py:3737: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 1150.7096774193549\n",
      "diff: 0.0\n",
      "Epoch 1 loss: 1096.258064516129\n",
      "diff: 0.0\n",
      "Epoch 2 loss: 1070.967741935484\n",
      "diff: 0.0\n",
      "Epoch 3 loss: 1052.3870967741937\n",
      "diff: 0.0\n",
      "Epoch 4 loss: 1042.3225806451612\n",
      "diff: 0.0\n",
      "Epoch 5 loss: 1029.2903225806451\n",
      "diff: 0.0\n",
      "Epoch 6 loss: 1021.4193548387096\n",
      "diff: 0.0\n",
      "Epoch 7 loss: 1019.3548387096774\n",
      "diff: 0.0\n",
      "Epoch 8 loss: 1016.0\n",
      "diff: 0.0\n",
      "Epoch 9 loss: 1013.9354838709677\n",
      "diff: 0.0\n",
      "Epoch 10 loss: 1013.6774193548387\n",
      "diff: 0.0\n",
      "Epoch 11 loss: 1011.3548387096774\n",
      "diff: 0.0\n",
      "Epoch 12 loss: 1009.0322580645161\n",
      "diff: 0.0\n",
      "Epoch 13 loss: 1007.6129032258065\n",
      "diff: 0.0\n",
      "Epoch 14 loss: 1011.0967741935484\n",
      "diff: 3.48387096774195\n",
      "Epoch 15 loss: 1008.516129032258\n",
      "diff: 0.9032258064515872\n",
      "Epoch 16 loss: 1007.0967741935484\n",
      "diff: 0.0\n",
      "Epoch 17 loss: 1007.8709677419355\n",
      "diff: 0.7741935483870748\n",
      "Epoch 18 loss: 1006.7096774193549\n",
      "diff: 0.0\n",
      "Epoch 19 loss: 1006.8387096774194\n",
      "diff: 0.12903225806451246\n",
      "Epoch 20 loss: 1007.8709677419355\n",
      "diff: 1.1612903225806122\n",
      "Epoch 21 loss: 1005.8064516129032\n",
      "diff: 0.0\n",
      "Epoch 22 loss: 1006.9677419354839\n",
      "diff: 1.1612903225807258\n",
      "Epoch 23 loss: 1005.9354838709677\n",
      "diff: 0.12903225806451246\n",
      "Epoch 24 loss: 1007.3548387096774\n",
      "diff: 1.5483870967742632\n",
      "Epoch 25 loss: 1008.0\n",
      "diff: 2.1935483870968255\n",
      "Epoch 26 loss: 1005.9354838709677\n",
      "diff: 0.12903225806451246\n",
      "Epoch 27 loss: 1006.0645161290323\n",
      "diff: 0.2580645161291386\n",
      "Epoch 28 loss: 1008.258064516129\n",
      "diff: 2.4516129032258505\n",
      "Epoch 29 loss: 1006.8387096774194\n",
      "diff: 1.0322580645162134\n",
      "Epoch 30 loss: 1007.6129032258065\n",
      "diff: 1.8064516129032882\n",
      "Epoch 31 loss: 1005.9354838709677\n",
      "diff: 0.12903225806451246\n",
      "Epoch 32 loss: 1004.9032258064516\n",
      "diff: 0.0\n",
      "Epoch 33 loss: 1005.4193548387096\n",
      "diff: 0.5161290322580498\n",
      "Epoch 34 loss: 1005.5483870967741\n",
      "diff: 0.6451612903225623\n",
      "Epoch 35 loss: 1005.9354838709677\n",
      "diff: 1.0322580645160997\n",
      "Epoch 36 loss: 1005.0322580645161\n",
      "diff: 0.12903225806451246\n",
      "Epoch 37 loss: 1005.0322580645161\n",
      "diff: 0.12903225806451246\n",
      "Epoch 38 loss: 1004.1290322580645\n",
      "diff: 0.0\n",
      "Epoch 39 loss: 1004.6451612903226\n",
      "diff: 0.5161290322580498\n",
      "Epoch 40 loss: 1005.1612903225806\n",
      "diff: 1.0322580645160997\n",
      "Epoch 41 loss: 1004.1290322580645\n",
      "diff: 0.0\n",
      "Epoch 42 loss: 1004.516129032258\n",
      "diff: 0.3870967741935374\n",
      "Epoch 43 loss: 1005.8064516129032\n",
      "diff: 1.677419354838662\n",
      "Epoch 44 loss: 1003.0967741935484\n",
      "diff: 0.0\n",
      "Epoch 45 loss: 1003.483870967742\n",
      "diff: 0.3870967741935374\n",
      "Epoch 46 loss: 1004.9032258064516\n",
      "diff: 1.8064516129031745\n",
      "Epoch 47 loss: 1007.6129032258065\n",
      "diff: 4.51612903225805\n",
      "Epoch 48 loss: 1004.1290322580645\n",
      "diff: 1.0322580645160997\n",
      "Epoch 49 loss: 1003.741935483871\n",
      "diff: 0.6451612903225623\n",
      "Epoch 50 loss: 1004.1290322580645\n",
      "diff: 1.0322580645160997\n",
      "Epoch 51 loss: 1004.258064516129\n",
      "diff: 1.1612903225806122\n",
      "Epoch 52 loss: 1004.7741935483871\n",
      "diff: 1.677419354838662\n",
      "Epoch 53 loss: 1004.7741935483871\n",
      "diff: 1.677419354838662\n",
      "Epoch 54 loss: 1005.1612903225806\n",
      "diff: 2.0645161290321994\n",
      "Epoch 55 loss: 1004.7741935483871\n",
      "diff: 1.677419354838662\n",
      "Epoch 56 loss: 1003.741935483871\n",
      "diff: 0.6451612903225623\n",
      "Epoch 57 loss: 1003.6129032258065\n",
      "diff: 0.5161290322580498\n",
      "Epoch 58 loss: 1004.1290322580645\n",
      "diff: 1.0322580645160997\n",
      "Epoch 59 loss: 1003.741935483871\n",
      "diff: 0.6451612903225623\n",
      "Epoch 60 loss: 1003.8709677419355\n",
      "diff: 0.7741935483870748\n",
      "Epoch 61 loss: 1003.741935483871\n",
      "diff: 0.6451612903225623\n",
      "Epoch 62 loss: 1003.741935483871\n",
      "diff: 0.6451612903225623\n",
      "Epoch 63 loss: 1003.8709677419355\n",
      "diff: 0.7741935483870748\n",
      "Epoch 64 loss: 1003.8709677419355\n",
      "diff: 0.7741935483870748\n",
      "Epoch 65 loss: 1002.8387096774194\n",
      "diff: 0.0\n",
      "Epoch 66 loss: 1003.8709677419355\n",
      "diff: 1.0322580645160997\n",
      "Epoch 67 loss: 1002.8387096774194\n",
      "diff: 0.0\n",
      "Epoch 68 loss: 1003.8709677419355\n",
      "diff: 1.0322580645160997\n",
      "Epoch 69 loss: 1001.6774193548387\n",
      "diff: 0.0\n",
      "Epoch 70 loss: 1004.258064516129\n",
      "diff: 2.580645161290363\n",
      "Epoch 71 loss: 1004.0\n",
      "diff: 2.322580645161338\n",
      "Epoch 72 loss: 1003.2258064516129\n",
      "diff: 1.5483870967742632\n",
      "Epoch 73 loss: 1002.9677419354839\n",
      "diff: 1.2903225806452383\n",
      "Epoch 74 loss: 1002.9677419354839\n",
      "diff: 1.2903225806452383\n",
      "Epoch 75 loss: 1005.8064516129032\n",
      "diff: 4.1290322580645125\n",
      "Epoch 76 loss: 1004.0\n",
      "diff: 2.322580645161338\n",
      "Epoch 77 loss: 1002.8387096774194\n",
      "diff: 1.1612903225807258\n",
      "Epoch 78 loss: 1002.5806451612904\n",
      "diff: 0.9032258064517009\n",
      "Epoch 79 loss: 1001.5483870967741\n",
      "diff: 0.0\n",
      "Epoch 80 loss: 1001.2903225806451\n",
      "diff: 0.0\n",
      "Epoch 81 loss: 1001.8064516129032\n",
      "diff: 0.5161290322580498\n",
      "Epoch 82 loss: 1003.0967741935484\n",
      "diff: 1.8064516129032882\n",
      "Epoch 83 loss: 1002.1935483870968\n",
      "diff: 0.9032258064517009\n",
      "Epoch 84 loss: 1001.5483870967741\n",
      "diff: 0.2580645161290249\n",
      "Epoch 85 loss: 1002.5806451612904\n",
      "diff: 1.2903225806452383\n",
      "Epoch 86 loss: 1002.3225806451613\n",
      "diff: 1.0322580645162134\n",
      "Epoch 87 loss: 1003.2258064516129\n",
      "diff: 1.9354838709678006\n",
      "Epoch 88 loss: 1004.3870967741935\n",
      "diff: 3.0967741935484128\n",
      "Epoch 89 loss: 1000.7741935483871\n",
      "diff: 0.0\n",
      "Epoch 90 loss: 1002.4516129032259\n",
      "diff: 1.6774193548387757\n",
      "Epoch 91 loss: 1001.9354838709677\n",
      "diff: 1.1612903225806122\n",
      "Epoch 92 loss: 1002.0645161290323\n",
      "diff: 1.2903225806452383\n",
      "Epoch 93 loss: 1000.3870967741935\n",
      "diff: 0.0\n",
      "Epoch 94 loss: 1004.3870967741935\n",
      "diff: 4.0\n",
      "Epoch 95 loss: 1001.6774193548387\n",
      "diff: 1.2903225806451246\n",
      "Epoch 96 loss: 1002.3225806451613\n",
      "diff: 1.9354838709678006\n",
      "Epoch 97 loss: 1002.7096774193549\n",
      "diff: 2.322580645161338\n",
      "Epoch 98 loss: 1001.5483870967741\n",
      "diff: 1.1612903225806122\n",
      "Epoch 99 loss: 1001.1612903225806\n",
      "diff: 0.7741935483870748\n"
     ]
    }
   ],
   "source": [
    "train_fn = lambda x:train(x,model,train_csp_dataset,100)\n",
    "diffusion_model = FABRIC.launch(train_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
